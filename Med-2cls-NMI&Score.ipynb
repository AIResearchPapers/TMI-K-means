{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9rEOBRWFY04v"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import io\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import normalized_mutual_info_score, pair_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fm_VSk_sZUfS"
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def DMM(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    data_standard = scaler.fit_transform(data)\n",
    "    return data_standard\n",
    "\n",
    "def DSD(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_standard = scaler.fit_transform(data)\n",
    "    return data_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "e7MrdIxmZUrL"
   },
   "outputs": [],
   "source": [
    "# Evaluation criterion\n",
    "def get_rand_index_and_f_measure(labels_true, labels_pred, beta=1.):\n",
    "  (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\n",
    "  ri = (tp + tn) / (tp + tn + fp + fn)\n",
    "  p, r = tp / (tp + fp), tp / (tp + fn)\n",
    "  sp = tn / (tn + fp)\n",
    "  f_beta = (1 + beta**2) * (p * r / ((beta ** 2) * p + r))\n",
    "  gmean = m.sqrt(r * sp)\n",
    "  return ri, p, r, sp, f_beta, gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hRxU2XA9ZUth"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "def H(X):\n",
    "    ''' calculate mutual entropy from observation of ramdom variable\n",
    "    '''\n",
    "    return calculate_ent_from_observation(X)\n",
    "\n",
    "def H2(X, Y):\n",
    "    '''\n",
    "        calculat Joint entropy of two ramdom variables\n",
    "    '''\n",
    "    return calculate_ent_from_observation(list(zip(X, Y)))\n",
    "\n",
    "def calculate_ent_from_observation(observation):\n",
    "    ''' input [a, a, b, c], return 1.5 '''\n",
    "\n",
    "    c = Counter(observation)  # Counting\n",
    "    l = len(observation)\n",
    "    distribution = []\n",
    "    for v in c.values():\n",
    "        distribution.append(v / l)\n",
    "\n",
    "    if abs(sum(distribution) - 1) > 0.0000001:\n",
    "        raise RuntimeError('sum of a distribution is not 1!')\n",
    "    return calculate_ent(distribution)\n",
    "\n",
    "# Information Entropy H(x)=-∑p(x)log p(x)\n",
    "def calculate_ent(distribution):\n",
    "    ''' example: input [0.5, 0.25, 0.25], return 1.5 '''\n",
    "\n",
    "    if abs(sum(distribution) - 1) > 0.0000001:\n",
    "        raise RuntimeError('sum of a distribution is not 1!')\n",
    "    ent = 0\n",
    "    for p in distribution:\n",
    "        ent -= p * m.log(p, 2)\n",
    "    return ent\n",
    "\n",
    "# Clustering through mutual information\n",
    "def cal_mi(data, clu, k):\n",
    "    \"\"\"\n",
    "    Calculate the distance between centroids and sample points.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Distance matrix between centroids and sample points\n",
    "    \"\"\"\n",
    "    dis = []\n",
    "    for i in range(len(data)):\n",
    "        dis.append([])\n",
    "        for j in range(k):\n",
    "            dis[i].append(2*(H(data[i])+H(clu[j])-H2(data[i],clu[j]))/(H(data[i])+H(clu[j])))\n",
    "            #dis[i].append(normalized_mutual_info_score(data[i], clu[j]))\n",
    "    return np.asarray(dis)\n",
    "\n",
    "def divide(data, dis):\n",
    "    \"\"\"\n",
    "    Group the sample points.\n",
    "    :param data: Set of sample points\n",
    "    :param dis: Distance from the centroids to all samples\n",
    "    :param k: Number of classes\n",
    "    :return: Set of Samples grouped\n",
    "    \"\"\"\n",
    "    clusterRes = [0] * len(data) # An array of '0' elements of length 'len(data)'\n",
    "    for i in range(len(data)):\n",
    "        seq = np.argsort(dis[i]) # In ascending order of distance\n",
    "        clusterRes[i] = seq[-1] # The last element in 'seq' joins 'clusterRes'\n",
    "\n",
    "    return np.asarray(clusterRes)\n",
    "\n",
    "def center(data, clusterRes, k):\n",
    "    \"\"\"\n",
    "    Calculate the centroids.\n",
    "    :param group: Set of samples grouped\n",
    "    :param k: Number of classes\n",
    "    :return: Centroids calculated\n",
    "    \"\"\"\n",
    "    clunew = []\n",
    "    for i in range(k):\n",
    "        # Calculate the new centroid for each group\n",
    "        idx = np.where(clusterRes == i)\n",
    "        sum = data[idx].sum(axis=0)\n",
    "        avg_sum = sum/len(data[idx])\n",
    "        clunew.append(avg_sum)\n",
    "    clunew = np.array(clunew)\n",
    "    return clunew[:, 0:]\n",
    "\n",
    "def classfy_mi(data, clu, k):\n",
    "    \"\"\"\n",
    "    Update centroids iteratively.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Error and new centroids\n",
    "    \"\"\"\n",
    "    clulist = cal_mi(data, clu, k)\n",
    "    clusterRes = divide(data, clulist)\n",
    "    clunew = center(data, clusterRes, k)\n",
    "    err = clunew - clu\n",
    "    return err, clunew, k, clusterRes\n",
    "\n",
    "#2.距离聚类\n",
    "def cal_dis(data, clu, k):\n",
    "    \"\"\"\n",
    "    Calculate the distance between centroids and sample points.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Distance matrix between centroids and sample points\n",
    "    \"\"\"\n",
    "    dis = []\n",
    "    for i in range(len(data)):\n",
    "        dis.append([])\n",
    "        for j in range(k):\n",
    "            dis[i].append(np.sqrt(np.sum((data[i]-clu[j])**2)))\n",
    "    return np.asarray(dis)\n",
    "\n",
    "def classfy_dis(data, clu, k):\n",
    "    \"\"\"\n",
    "    Update centroids iteratively.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Error and new centroids\n",
    "    \"\"\"\n",
    "    clulist = cal_dis(data, clu, k)\n",
    "    clusterRes = divide(data, clulist)\n",
    "    clunew = center(data, clusterRes, k)\n",
    "    err = clunew - clu\n",
    "    return err, clunew, k, clusterRes\n",
    "\n",
    "#3.曼哈顿聚类\n",
    "def cal_Manhattan(data, clu, k):\n",
    "    \"\"\"\n",
    "    Calculate the distance between centroids and sample points.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Distance matrix between centroids and sample points\n",
    "    \"\"\"\n",
    "    dis = []\n",
    "    for i in range(len(data)):\n",
    "        dis.append([])\n",
    "        for j in range(k):\n",
    "            dis[i].append(np.sum(abs(data[i]-clu[j])))\n",
    "    return np.asarray(dis)\n",
    "\n",
    "def classfy_Manhattan(data, clu, k):\n",
    "    \"\"\"\n",
    "    Update centroids iteratively.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Error and new centroids\n",
    "    \"\"\"\n",
    "    clulist = cal_Manhattan(data, clu, k)\n",
    "    clusterRes = divide(data, clulist)\n",
    "    clunew = center(data, clusterRes, k)\n",
    "    err = clunew - clu\n",
    "    return err, clunew, k, clusterRes\n",
    "\n",
    "#4.q=3的明可夫斯基距离聚类\n",
    "def cal_Minkowski_3(data, clu, k):\n",
    "    \"\"\"\n",
    "    Calculate the distance between centroids and sample points.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Distance matrix between centroids and sample points\n",
    "    \"\"\"\n",
    "    dis = []\n",
    "    for i in range(len(data)):\n",
    "        dis.append([])\n",
    "        for j in range(k):\n",
    "            dis[i].append(pow(np.sum((data[i]-clu[j])**3),1/3))\n",
    "    return np.asarray(dis)\n",
    "\n",
    "def classfy_Minkowski_3(data, clu, k):\n",
    "    \"\"\"\n",
    "    Update centroids iteratively.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Error and new centroids\n",
    "    \"\"\"\n",
    "    clulist = cal_Minkowski_3(data, clu, k)\n",
    "    clusterRes = divide(data, clulist)\n",
    "    clunew = center(data, clusterRes, k)\n",
    "    err = clunew - clu\n",
    "    return err, clunew, k, clusterRes\n",
    "\n",
    "#5.q=4的明可夫斯基距离聚类\n",
    "def cal_Minkowski_4(data, clu, k):\n",
    "    \"\"\"\n",
    "    Calculate the distance between centroids and sample points.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Distance matrix between centroids and sample points\n",
    "    \"\"\"\n",
    "    dis = []\n",
    "    for i in range(len(data)):\n",
    "        dis.append([])\n",
    "        for j in range(k):\n",
    "            dis[i].append(pow(np.sum((data[i]-clu[j])**4),1/4))\n",
    "    return np.asarray(dis)\n",
    "\n",
    "def classfy_Minkowski_4(data, clu, k):\n",
    "    \"\"\"\n",
    "    Update centroids iteratively.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Error and new centroids\n",
    "    \"\"\"\n",
    "    clulist = cal_Minkowski_4(data, clu, k)\n",
    "    clusterRes = divide(data, clulist)\n",
    "    clunew = center(data, clusterRes, k)\n",
    "    err = clunew - clu\n",
    "    return err, clunew, k, clusterRes\n",
    "\n",
    "#6.q=5的明可夫斯基距离聚类\n",
    "def cal_Minkowski_5(data, clu, k):\n",
    "    \"\"\"\n",
    "    Calculate the distance between centroids and sample points.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Distance matrix between centroids and sample points\n",
    "    \"\"\"\n",
    "    dis = []\n",
    "    for i in range(len(data)):\n",
    "        dis.append([])\n",
    "        for j in range(k):\n",
    "            dis[i].append(pow(np.sum((data[i]-clu[j])**5),1/5))\n",
    "    return np.asarray(dis)\n",
    "\n",
    "def classfy_Minkowski_5(data, clu, k):\n",
    "    \"\"\"\n",
    "    Update centroids iteratively.\n",
    "    :param data: Set of sample points\n",
    "    :param clu: Set of centroids\n",
    "    :param k: Number of classes\n",
    "    :return: Error and new centroids\n",
    "    \"\"\"\n",
    "    clulist = cal_Minkowski_5(data, clu, k)\n",
    "    clusterRes = divide(data, clulist)\n",
    "    clunew = center(data, clusterRes, k)\n",
    "    err = clunew - clu\n",
    "    return err, clunew, k, clusterRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Q6ciX1Y7fYqj"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dataOri = pd.read_csv('/content/crf_base_dataset-fc.csv')\n",
    "#data.replace('?', np.nan, inplace=True)\n",
    "#column_means = data.apply(pd.to_numeric, errors='coerce').mean()\n",
    "#data.fillna(column_means, inplace=True)\n",
    "Target = np.array(dataOri.iloc[:,-1].values,dtype=float)\n",
    "data = np.array(dataOri.iloc[:,1:].values,dtype=float)\n",
    "data = DMM(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zoc9IMmX5eNr"
   },
   "outputs": [],
   "source": [
    "# Deal with uneven and asymmetrical data distribution\n",
    "#count_0 = data['Atrial_fibrillation'].value_counts()[0]#是否复发\n",
    "#count_1 = data['Atrial_fibrillation'].value_counts()[1]\n",
    "\n",
    "#sampled_0 = data[data['Atrial_fibrillation'] == 0].sample(n=count_1,random_state=42)\n",
    "#sampled_1 = data[data['Atrial_fibrillation'] == 1].sample(n=count_1,random_state=42)\n",
    "\n",
    "#sampled_df = pd.concat([sampled_0, sampled_1])\n",
    "\n",
    "#Target = np.array(sampled_df.iloc[:,-1].values,dtype=float)\n",
    "#data = sampled_df.iloc[:,1:]\n",
    "#data = DMM(data)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG1rygEY9Bx5"
   },
   "source": [
    "1.Experiments between our method with other clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AtIK8kPE3mA"
   },
   "outputs": [],
   "source": [
    "#Kmeans\n",
    "\n",
    "k = 2 # num_centroids\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(data)\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "mi = []\n",
    "meanmi = [0] * k\n",
    "counter = [0] * k\n",
    "for i in range(len(data)):\n",
    "  j = labels[i]\n",
    "  mi.append(2*(H(data[i])+H(cluster_centers[j])-H2(data[i],cluster_centers[j]))/(H(data[i])+H(cluster_centers[j])))\n",
    "  meanmi[j] += mi[i]\n",
    "  counter[j] += 1\n",
    "\n",
    "for i in range(k):\n",
    "  if counter[i] == 0:\n",
    "    meanmi[i] = 0\n",
    "  else:\n",
    "    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "ri, p, r, f_beta = get_rand_index_and_f_measure(Target,labels,beta=1.)\n",
    "print(f\"\\nri:{ri}\\np:{p}\\nr:{r}\\nf_measure:{f_beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ALkcxzxM79G"
   },
   "outputs": [],
   "source": [
    "# TSNE+Kmeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=3)\n",
    "lowdata = tsne.fit_transform(data)\n",
    "\n",
    "km = kmeans.fit(lowdata)\n",
    "\n",
    "cluster_centers = km.cluster_centers_\n",
    "labels = km.labels_\n",
    "\n",
    "mi = []\n",
    "meanmi = [0] * k\n",
    "counter = [0] * k\n",
    "for i in range(len(lowdata)):\n",
    "  j = labels[i]\n",
    "  mi.append(2*(H(lowdata[i])+H(cluster_centers[j])-H2(lowdata[i],cluster_centers[j]))/(H(lowdata[i])+H(cluster_centers[j])))\n",
    "  meanmi[j] += mi[i]\n",
    "  counter[j] += 1\n",
    "\n",
    "for i in range(k):\n",
    "  if counter[i] == 0:\n",
    "    meanmi[i] = 0\n",
    "  else:\n",
    "    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "ri, p, r, f_beta = get_rand_index_and_f_measure(Target,labels,beta=1.)\n",
    "print(f\"\\nri:{ri}\\np:{p}\\nr:{r}\\nf_measure:{f_beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1uzQzi1vfRs"
   },
   "outputs": [],
   "source": [
    "# AgglomerativeClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=k)\n",
    "\n",
    "labels = agg.fit_predict(data)\n",
    "\n",
    "ri, p, r, f_beta = get_rand_index_and_f_measure(Target,labels,beta=1.)\n",
    "print(f\"\\nri:{ri}\\np:{p}\\nr:{r}\\nf_measure:{f_beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edX0QxmDNFOP"
   },
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(min_samples=100)\n",
    "\n",
    "labels = dbscan.fit_predict(data)\n",
    "\n",
    "ri, p, r, f_beta = get_rand_index_and_f_measure(Target,labels,beta=1.)\n",
    "print(f\"\\nri:{ri}\\np:{p}\\nr:{r}\\nf_measure:{f_beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N5r-Bqr1czGF"
   },
   "outputs": [],
   "source": [
    "# Rank characteristics from small to large according to mutual information indicators\n",
    "def Rank(da):\n",
    "  mi = []\n",
    "  for i in range(len(da)):\n",
    "    mi.append([])\n",
    "    for j in range(len(da)):\n",
    "      mi[i].append(normalized_mutual_info_score(da[i], da[j]))\n",
    "  minmi = np.sum(mi,axis=0)\n",
    "  seq = np.argsort(minmi)\n",
    "  return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIfXgEfd9XA4"
   },
   "source": [
    "2.Experiments on Tensor K-means with different measures (with k-fold cross-validation and indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "TJOrJ0ji2C59"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = np.array(dataOri.iloc[:,1:].values)\n",
    "X = DMM(X)\n",
    "Y = np.array(dataOri.iloc[:,-1].values)\n",
    "\n",
    "kf = KFold(10, shuffle=False)\n",
    "for train_index, test_index in kf.split(X):\n",
    "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "  k = 2 # num_centroids\n",
    "  seq = Rank(X_train)\n",
    "\n",
    "  #1.TMI\n",
    "  clu = X_train[seq[0:k]]\n",
    "  clu = np.asarray(clu)\n",
    "  err, clunew, k, clusterRes = classfy_mi(X_train, clu, k)\n",
    "  iter = 0\n",
    "  while np.any(abs(err) > 0):\n",
    "    err, clunew, k, clusterRes = classfy_mi(X_train, clunew, k)\n",
    "    iter += 1\n",
    "    if np.all(abs(err) < 0.01):\n",
    "      break\n",
    "\n",
    "  milist = cal_mi(X_train, clunew, k)\n",
    "  TrainResult = divide(X_train, milist)\n",
    "  ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y_train,TrainResult,beta=1.)\n",
    "  print(f\"TMI-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "  #print(classification_report(Y_train.astype(TrainResult.dtype), TrainResult, digits=4))\n",
    "  #print(TrainResult,'\\n')\n",
    "\n",
    "  milist = cal_mi(X_test, clunew, k)\n",
    "  TestResult = divide(X_test, milist)\n",
    "  ri2, p2, r2, sp2, fscore2, gmean2 = get_rand_index_and_f_measure(Y_test,TestResult,beta=1.)\n",
    "  print(f\"TMI-Test: \\n{ri2}\\n{p2}\\n{r2}\\n{sp2}\\n{fscore2}\\n{gmean2}\")\n",
    "  #print(classification_report(Y_test.astype(TestResult.dtype), TestResult, digits=4))\n",
    "  #print(TestResult,'\\n')\n",
    "\n",
    "  #2.Euclidean\n",
    "  clu = X_train[seq[0:k]]\n",
    "  clu = np.asarray(clu)\n",
    "  err, clunew, k, clusterRes = classfy_dis(X_train, clu, k)\n",
    "  iter = 0\n",
    "  while np.any(abs(err) > 0):\n",
    "    err, clunew, k, clusterRes = classfy_dis(X_train, clunew, k)\n",
    "    iter += 1\n",
    "    if np.all(abs(err) < 0.01):\n",
    "      break\n",
    "\n",
    "  milist = cal_dis(X_train, clunew, k)\n",
    "  TrainResult = divide(X_train, milist)\n",
    "  ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y_train,TrainResult,beta=1.)\n",
    "  print(f\"EUC-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "  #print(TrainResult,'\\n')\n",
    "\n",
    "  milist = cal_dis(X_test, clunew, k)\n",
    "  TestResult = divide(X_test, milist)\n",
    "  ri2, p2, r2, sp2, fscore2, gmean2 = get_rand_index_and_f_measure(Y_test,TestResult,beta=1.)\n",
    "  print(f\"EUC-Test: \\n{ri2}\\n{p2}\\n{r2}\\n{sp2}\\n{fscore2}\\n{gmean2}\")\n",
    "  #print(TestResult,'\\n')\n",
    "\n",
    "  #3.MANHATTEN\n",
    "  clu = X_train[seq[0:k]]\n",
    "  clu = np.asarray(clu)\n",
    "  err, clunew, k, clusterRes = classfy_Manhattan(X_train, clu, k)\n",
    "  iter = 0\n",
    "  while np.any(abs(err) > 0):\n",
    "    err, clunew, k, clusterRes = classfy_Manhattan(X_train, clunew, k)\n",
    "    iter += 1\n",
    "    if np.all(abs(err) < 0.01):\n",
    "      break\n",
    "\n",
    "  milist = cal_Manhattan(X_train, clunew, k)\n",
    "  TrainResult = divide(X_train, milist)\n",
    "  ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y_train,TrainResult,beta=1.)\n",
    "  print(f\"MHT-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "  #print(TrainResult,'\\n')\n",
    "\n",
    "  milist = cal_Manhattan(X_test, clunew, k)\n",
    "  TestResult = divide(X_test, milist)\n",
    "  ri2, p2, r2, sp2, fscore2, gmean2 = get_rand_index_and_f_measure(Y_test,TestResult,beta=1.)\n",
    "  print(f\"MHT-Test: \\n{ri2}\\n{p2}\\n{r2}\\n{sp2}\\n{fscore2}\\n{gmean2}\")\n",
    "  #print(TestResult,'\\n')\n",
    "\n",
    "  #4.q=3\n",
    "  clu = X_train[seq[0:k]]\n",
    "  clu = np.asarray(clu)\n",
    "  err, clunew, k, clusterRes = classfy_Minkowski_3(X_train, clu, k)\n",
    "  iter = 0\n",
    "  while np.any(abs(err) > 0):\n",
    "    err, clunew, k, clusterRes = classfy_Minkowski_3(X_train, clunew, k)\n",
    "    iter += 1\n",
    "    if np.all(abs(err) < 0.01):\n",
    "      break\n",
    "\n",
    "  milist = cal_Minkowski_3(X_train, clunew, k)\n",
    "  TrainResult = divide(X_train, milist)\n",
    "  ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y_train,TrainResult,beta=1.)\n",
    "  print(f\"3-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "  #print(TrainResult,'\\n')\n",
    "\n",
    "  milist = cal_Minkowski_3(X_test, clunew, k)\n",
    "  TestResult = divide(X_test, milist)\n",
    "  ri2, p2, r2, sp2, fscore2, gmean2 = get_rand_index_and_f_measure(Y_test,TestResult,beta=1.)\n",
    "  print(f\"3-Test: \\n{ri2}\\n{p2}\\n{r2}\\n{sp2}\\n{fscore2}\\n{gmean2}\")\n",
    "  #print(TestResult,'\\n')\n",
    "\n",
    "  #5.q=4\n",
    "  clu = X_train[seq[0:k]]\n",
    "  clu = np.asarray(clu)\n",
    "  err, clunew, k, clusterRes = classfy_Minkowski_4(X_train, clu, k)\n",
    "  iter = 0\n",
    "  while np.any(abs(err) > 0):\n",
    "    err, clunew, k, clusterRes = classfy_Minkowski_4(X_train, clunew, k)\n",
    "    iter += 1\n",
    "    if np.all(abs(err) < 0.01):\n",
    "      break\n",
    "\n",
    "  milist = cal_Minkowski_4(X_train, clunew, k)\n",
    "  TrainResult = divide(X_train, milist)\n",
    "  ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y_train,TrainResult,beta=1.)\n",
    "  print(f\"4-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "  #print(TrainResult,'\\n')\n",
    "\n",
    "  milist = cal_Minkowski_4(X_test, clunew, k)\n",
    "  TestResult = divide(X_test, milist)\n",
    "  ri2, p2, r2, sp2, fscore2, gmean2 = get_rand_index_and_f_measure(Y_test,TestResult,beta=1.)\n",
    "  print(f\"4-Test: \\n{ri2}\\n{p2}\\n{r2}\\n{sp2}\\n{fscore2}\\n{gmean2}\")\n",
    "  #print(TestResult,'\\n')\n",
    "\n",
    "  #6.q=6\n",
    "  clu = X_train[seq[0:k]]\n",
    "  clu = np.asarray(clu)\n",
    "  err, clunew, k, clusterRes = classfy_Minkowski_5(X_train, clu, k)\n",
    "  iter = 0\n",
    "  while np.any(abs(err) > 0):\n",
    "    err, clunew, k, clusterRes = classfy_Minkowski_5(X_train, clunew, k)\n",
    "    iter += 1\n",
    "    if np.all(abs(err) < 0.01):\n",
    "      break\n",
    "\n",
    "  milist = cal_Minkowski_5(X_train, clunew, k)\n",
    "  TrainResult = divide(X_train, milist)\n",
    "  ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y_train,TrainResult,beta=1.)\n",
    "  print(f\"5-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "  #print(TrainResult,'\\n')\n",
    "\n",
    "  milist = cal_Minkowski_5(X_test, clunew, k)\n",
    "  TestResult = divide(X_test, milist)\n",
    "  ri2, p2, r2, sp2, fscore2, gmean2 = get_rand_index_and_f_measure(Y_test,TestResult,beta=1.)\n",
    "  print(f\"5-Test: \\n{ri2}\\n{p2}\\n{r2}\\n{sp2}\\n{fscore2}\\n{gmean2}\")\n",
    "  #print(TestResult,'\\n')\n",
    "\n",
    "  print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en7rwf7z-Nfz"
   },
   "source": [
    "3.Experiments on Tensor K-means with different measures (focusing on the NMI indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9Zkj1sjc5rN"
   },
   "outputs": [],
   "source": [
    "# The effect of mutual information index clustering\n",
    "k = 2 # num_centroids\n",
    "Y = np.array(data.iloc[:,-1].values)\n",
    "data = np.array(data.iloc[:,:].values)\n",
    "data = DMM(data)\n",
    "seq = Rank(data)\n",
    "clu = data[seq[0:k]]\n",
    "clu = np.asarray(clu)\n",
    "err, clunew, k, clusterRes = classfy_mi(data, clu, k)\n",
    "iter = 0\n",
    "while np.any(abs(err) > 0):\n",
    "  err, clunew, k, clusterRes = classfy_mi(data, clunew, k)\n",
    "  iter += 1\n",
    "  if np.any(abs(err) < 0.01):\n",
    "    break\n",
    "\n",
    "milist = cal_mi(data, clunew, k)\n",
    "clusterResult = divide(data, milist)\n",
    "\n",
    "print('Based on mutual information index, the cluster result of %d class is as follows.' % (k))\n",
    "print(clusterResult,'\\n')\n",
    "\n",
    "#print('The cluster center of %d class is as follows.' % (k))\n",
    "#print(clunew,'\\n')\n",
    "\n",
    "#mi = []\n",
    "#meanmi = [0] * k\n",
    "#counter = [0] * k\n",
    "#for i in range(len(X)):\n",
    "#  j = clusterResult[i]\n",
    "#  mi.append(2*(H(X[i])+H(clunew[j])-H2(X[i],clunew[j]))/(H(X[i])+H(clunew[j])))\n",
    "#  meanmi[j] += mi[i]\n",
    "#  counter[j] += 1\n",
    "\n",
    "#for i in range(k):\n",
    "#  if counter[i] == 0:\n",
    "#    meanmi[i] = 0\n",
    "#  else:\n",
    "#    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "#print('The clustering mutual information evaluation index of %d class is as follows.' % (k))\n",
    "#print(np.mean(meanmi),'\\n')\n",
    "\n",
    "ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y,clusterResult,beta=1.)\n",
    "print(f\"TMI-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "#print(classification_report(Y, clusterResult, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZEwsfngdXkl"
   },
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "clu = data[seq[0:k]]\n",
    "clu = np.asarray(clu)\n",
    "err, clunew, k, clusterRes = classfy_dis(data, clu, k)\n",
    "iter = 0\n",
    "while np.any(abs(err) > 0):\n",
    "  err, clunew, k, clusterRes = classfy_dis(data, clunew, k)\n",
    "  iter += 1\n",
    "  if np.any(abs(err) < 0.01):\n",
    "    break\n",
    "\n",
    "clulist = cal_dis(data, clunew, k)\n",
    "clusterResult = divide(data, clulist)\n",
    "\n",
    "print('The cluster center of %d class is as follows.' % (k))\n",
    "print(clunew,'\\n')\n",
    "\n",
    "mi = []\n",
    "meanmi = [0] * k\n",
    "counter = [0] * k\n",
    "for i in range(len(data)):\n",
    "  j = clusterResult[i]\n",
    "  mi.append(2*(H(data[i])+H(clunew[j])-H2(data[i],clunew[j]))/(H(data[i])+H(clunew[j])))\n",
    "  meanmi[j] += mi[i]\n",
    "  counter[j] += 1\n",
    "\n",
    "for i in range(k):\n",
    "  if counter[i] == 0:\n",
    "    meanmi[i] = 0\n",
    "  else:\n",
    "    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "print('The clustering mutual information evaluation index of %d class is as follows.' % (k))\n",
    "print(np.mean(meanmi),'\\n')\n",
    "\n",
    "ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y,clusterResult,beta=1.)\n",
    "print(f\"TMI-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "#print(classification_report(Y, clusterResult, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fwyf5FludYXd"
   },
   "outputs": [],
   "source": [
    "# Manhattan distance\n",
    "clu = data[seq[0:k]]\n",
    "clu = np.asarray(clu)\n",
    "err, clunew, k, clusterRes = classfy_Manhattan(data, clu, k)\n",
    "iter = 0\n",
    "while np.any(abs(err) > 0):\n",
    "  err, clunew, k, clusterRes = classfy_Manhattan(data, clunew, k)\n",
    "  iter += 1\n",
    "  if np.any(abs(err) < 0.01):\n",
    "    break\n",
    "\n",
    "clulist = cal_Manhattan(data, clunew, k)\n",
    "clusterResult = divide(data, clulist)\n",
    "\n",
    "print('The cluster center of %d class is as follows.' % (k))\n",
    "print(clunew,'\\n')\n",
    "\n",
    "mi = []\n",
    "meanmi = [0] * k\n",
    "counter = [1] * k\n",
    "for i in range(len(data)):\n",
    "  j = clusterResult[i]\n",
    "  mi.append(2*(H(data[i])+H(clunew[j])-H2(data[i],clunew[j]))/(H(data[i])+H(clunew[j])))\n",
    "  meanmi[j] += mi[i]\n",
    "  counter[j] += 1\n",
    "\n",
    "for i in range(k):\n",
    "  if counter[i] == 0:\n",
    "    meanmi[i] = 0\n",
    "  else:\n",
    "    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "print('The clustering mutual information evaluation index of %d class is as follows.' % (k))\n",
    "print(np.mean(meanmi),'\\n')\n",
    "\n",
    "ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y,clusterResult,beta=1.)\n",
    "print(f\"TMI-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "#print(classification_report(Y, clusterResult, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9O9qorndZ20"
   },
   "outputs": [],
   "source": [
    "# Minkowski distance(q=3)\n",
    "clu = data[seq[0:k]]\n",
    "clu = np.asarray(clu)\n",
    "err, clunew, k, clusterRes = classfy_Minkowski_3(data, clu, k)\n",
    "iter = 0\n",
    "while np.any(abs(err) > 0):\n",
    "  err, clunew, k, clusterRes = classfy_Minkowski_3(data, clunew, k)\n",
    "  iter += 1\n",
    "  if np.any(abs(err) < 0.01):\n",
    "    break\n",
    "\n",
    "clulist = cal_Minkowski_3(data, clunew, k)\n",
    "clusterResult = divide(data, clulist)\n",
    "\n",
    "print('The cluster center of %d class is as follows.' % (k))\n",
    "print(clunew,'\\n')\n",
    "\n",
    "mi = []\n",
    "meanmi = [0] * k\n",
    "counter = [0] * k\n",
    "for i in range(len(data)):\n",
    "  j = clusterResult[i]\n",
    "  mi.append(2*(H(data[i])+H(clunew[j])-H2(data[i],clunew[j]))/(H(data[i])+H(clunew[j])))\n",
    "  meanmi[j] += mi[i]\n",
    "  counter[j] += 1\n",
    "\n",
    "for i in range(k):\n",
    "  if counter[i] == 0:\n",
    "    meanmi[i] = 0\n",
    "  else:\n",
    "    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "print('The clustering mutual information evaluation index of %d class is as follows.' % (k))\n",
    "print(np.mean(meanmi),'\\n')\n",
    "\n",
    "ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y,clusterResult,beta=1.)\n",
    "print(f\"TMI-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "#print(classification_report(Y, clusterResult, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVw0gIWVdcSY"
   },
   "outputs": [],
   "source": [
    "# Minkowski distance(q=4)\n",
    "clu = data[seq[0:k]]\n",
    "clu = np.asarray(clu)\n",
    "err, clunew, k, clusterRes = classfy_Minkowski_4(data, clu, k)\n",
    "iter = 0\n",
    "while np.any(abs(err) > 0):\n",
    "  err, clunew, k, clusterRes = classfy_Minkowski_4(data, clunew, k)\n",
    "  iter += 1\n",
    "  if np.any(abs(err) < 0.01):\n",
    "    break\n",
    "\n",
    "clulist = cal_Minkowski_4(data, clunew, k)\n",
    "clusterResult = divide(data, clulist)\n",
    "\n",
    "print('The cluster center of %d class is as follows.' % (k))\n",
    "print(clunew,'\\n')\n",
    "\n",
    "mi = []\n",
    "meanmi = [0] * k\n",
    "counter = [1] * k\n",
    "for i in range(len(data)):\n",
    "  j = clusterResult[i]\n",
    "  mi.append(2*(H(data[i])+H(clunew[j])-H2(data[i],clunew[j]))/(H(data[i])+H(clunew[j])))\n",
    "  meanmi[j] += mi[i]\n",
    "  counter[j] += 1\n",
    "\n",
    "for i in range(k):\n",
    "  if counter[i] == 0:\n",
    "    meanmi[i] = 0\n",
    "  else:\n",
    "    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "print('The clustering mutual information evaluation index of %d class is as follows.' % (k))\n",
    "print(np.mean(meanmi),'\\n')\n",
    "\n",
    "ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y,clusterResult,beta=1.)\n",
    "print(f\"TMI-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "#print(classification_report(Y, clusterResult, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkd2SrB2ddtA"
   },
   "outputs": [],
   "source": [
    "# Minkowski distance(q=5)\n",
    "clu = data[seq[0:k]]\n",
    "clu = np.asarray(clu)\n",
    "err, clunew, k, clusterRes = classfy_Minkowski_5(data, clu, k)\n",
    "iter = 0\n",
    "while np.any(abs(err) > 0):\n",
    "  err, clunew, k, clusterRes = classfy_Minkowski_5(data, clunew, k)\n",
    "  iter += 1\n",
    "  if np.any(abs(err) < 0.01):\n",
    "    break\n",
    "\n",
    "clulist = cal_Minkowski_5(data, clunew, k)\n",
    "clusterResult = divide(data, clulist)\n",
    "\n",
    "print('The cluster center of %d class is as follows.' % (k))\n",
    "print(clunew,'\\n')\n",
    "\n",
    "mi = []\n",
    "meanmi = [0] * k\n",
    "counter = [0] * k\n",
    "for i in range(len(data)):\n",
    "  j = clusterResult[i]\n",
    "  mi.append(2*(H(data[i])+H(clunew[j])-H2(data[i],clunew[j]))/(H(data[i])+H(clunew[j])))\n",
    "  meanmi[j] += mi[i]\n",
    "  counter[j] += 1\n",
    "\n",
    "for i in range(k):\n",
    "  if counter[i] == 0:\n",
    "    meanmi[i] = 0\n",
    "  else:\n",
    "    meanmi[i] = meanmi[i] / (counter[i])\n",
    "\n",
    "print('The clustering mutual information evaluation index of %d class is as follows.' % (k))\n",
    "print(np.mean(meanmi),'\\n')\n",
    "\n",
    "ri1, p1, r1, sp1, fscore1, gmean1 = get_rand_index_and_f_measure(Y,clusterResult,beta=1.)\n",
    "print(f\"TMI-Train: \\n{ri1}\\n{p1}\\n{r1}\\n{sp1}\\n{fscore1}\\n{gmean1}\")\n",
    "#print(classification_report(Y, clusterResult, digits=4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
